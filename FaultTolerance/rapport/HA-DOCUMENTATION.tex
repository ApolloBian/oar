\documentclass[a4paper,10pt]{report}
\usepackage[utf8]{inputenc} 
\usepackage{graphicx}
\usepackage[margin=2.5cm]{geometry}
% ---- inclusion de codes
\usepackage{listings} 
\usepackage{color}
\definecolor{lbcolor}{rgb}{0.95,0.95,0.95}
\lstset{frame=single, breaklines=true, backgroundcolor=\color{lbcolor}, basicstyle=\ttfamily,basicstyle=\scriptsize, keywordstyle=\color{red}, commentstyle=\color{vert}, stringstyle=\color{blue}, identifierstyle=\ttfamily}






% Title Page
\title{High Availability Documentation on OAR - Admin Guide\\
\includegraphics[scale=0.7]{schema/oar_logo.png}}
\author{Joris Br√©mond - \texttt{joris.bremond@gmail.com}\\
\\
Laboratoire d'Informatique de Grenoble\\
Bat. ENSIMAG - antenne de Montbonnot\\
ZIRST 51, avenue Jean Kuntzmann\\
38330 MONTBONNOT SAINT MARTIN\\
\\
Authors: LIG laboratory\\
Organization: LIG laboratory\\
Status: Testing\\
Copyright: licenced under the GNU GENERAL PUBLIC LICENSE\\
}




\begin{document}

\maketitle
\tableofcontents

\begin{abstract}
This documentation explain how to configure High Availability on OAR. This solution provide fault tolerance on both OAR server and database (mysql or postgresql).
\end{abstract}












  
\chapter{Introduction}
This documentation explain how to configure a high availability cluster with OAR. We will see to different configurations :
\begin{description}
\item[]- OAR-server daemon and database on the same server. So we have two servers, the master OAR-server and the backup.
(insert schema)
\item[]- OAR-server daemon and database on different server. We have four servers, the master OAR-server, the master database, the backup OAR-server, and the backup database.
(insert schema)
\end{description}


\section{Require}
This documentation don't describe how to configure and install OAR. So before install OAR High Availability solution, install OAR with the database of your choice (mysql or postgresql). OAR-server and database must be also installed on the backup(s) server(s), with the same daemon configuration (oar.conf, my.conf or pg.conf). Nevertheless, it's not necessary that we configure OAR (oarnodesetting, etc.) because it's data incorporated in database and DRBD will share it. So we will have the same configurations in both nodes.
Before continue, please stop oar-server and database daemon.



\section{General functioning}
For provide OAR High Availability, we use different produces :
\begin{description}
\item[]- The first one is \textbf{Heartbeat} version 2. Heartbeat is a daemon that provides fault tolerance. It manage and monitor services, like Virtual IP, filesystem, etc. When Heartbeat detect problems in resource, it can migrate this one on other server. It can manage groups of resources and define rules and priority on each server.
\item[]- \textbf{DRBD}. DRBD refers to block devices designed as a building block to form high availability clusters. This is done by mirroring a whole block device via an assigned network. DRBD can be understood as network based raid-1. In OAR high availability solution, we will use DRBD for duplicate database.\\
(insert schema)
\item[]- \textbf{VirtualIP} : This package is installed with heartbeat, and managed by it. It provide a virtual IP address. Thereby, client permanently know the elected server.

\end{description}













\chapter{Installation}
\section{DRBD}
In this guide, we use DRBD-8.
\begin{description}
\item[]- On debian :

The package name is drbd8-utils. It's easier to use module-assitant for install it:
\begin{lstlisting}
apt-get install drbd8-utils module-assistant
module-assistant auto-install drbd8
\end{lstlisting} 



Module Assitant require kernel header. You can install it with :
\begin{lstlisting}
apt-get install linux-headers-`uname -r`
\end{lstlisting}

\end{description}

Now DRBD is operational.


\section{Heartbeat-2}
Heartbeat has two version. We will use the version 2 because we can monitor each resource and detect service crash.
\begin{description}
\item[]- On debian:

The package name is heartbeat-2.
\begin{lstlisting}
apt-get install heartbeat-2
\end{lstlisting}
You can also install the GUI pakage on the your computer for heartbeat remote configuration.
\begin{lstlisting}
apt-get install heartbeat-2-gui
\end{lstlisting}
\end{description}











\chapter{Configuration}

For illustrate example, the nodes names are :
\begin{description}
\item[]- 2 nodes configurations : 
\begin{itemize}
\item node1 (master)
\item node2 (backup)
\end{itemize}

\item[]- 4 nodes configurations :
\begin{itemize}
\item node1 (master oar)
\item node2 (master database)
\item node3 (backup oar)
\item node4 (backup database)
\end{itemize}
\end{description}


\section{System configuration}
It's really recommended that we have two different network between each Heartbeat server. Indeed, heartbeat use the network for check its neighbour. With two networks, we can for example avoid split-brain problem. We can use one Ethernet network and one serial network, ore two Ethernet network. With a solution like this, we can easyly cut the Ethernet network without interfere with heartbeat functioning.\\
For Heartbeat configuration, we need one or two network address (Virtual IP). If we are in configuration 2 nodes (OAR-server and database on the same nodes), we need one IP address, and two IP if we are in 4 nodes configurations. This virtual IP must be free and accessible on the network.\\
DRBD need a low-level storage for write data. It could be an entire disk, a partition, or a virtual partition (in a file). So before configure DRBD, create a free partition.\\
If you want create a partition in a file (/image.img), you can follow this commands
\begin{lstlisting}
SIZE=200	#Size in Mega-bytes
dd if=/dev/zero of=/image.img bs=1M count=$SIZE
losetup /dev/loop/0 /image.img
mkfs -t ext3 /dev/loop/0
shred -zvf -n 1 /dev/loop/0	
\end{lstlisting}



\section{Heartbeat Configuration}

There are three files to configure heartbeat-2 :
\subsection{Authentication file : \textit{/etc/ha.d/authkeys}}
The authkeys configuration file contains information for Heartbeat to use when authenticating cluster members. It cannot be readable or writable by anyone other than root. For sha authentification, write ::
\begin{lstlisting}
auth 1
1 sha1 MyPassword
\end{lstlisting}
You can also use \textbf{md5} or \textbf{crc}.\\
Set file right:
\begin{lstlisting}
chmod 0600 /etc/ha.d/authkeys
\end{lstlisting}

\subsection{Daemon configuration file : \textit{/etc/ha.d/ha.cf}}

The ha.cf file is one of the more important files. It lists the cluster nodes, the communications topology, and which features of the configuration are enabled. 
\begin{itemize}
\item - An example with 2 nodes configuration and two Ethernet interfaces ::
\begin{lstlisting}
logfile /var/log/ha-log
debugfile /var/log/ha-debug
#use_logd on
udpport 694
keepalive 1 # 1 second
deadtime 10
initdead 80
ucast eth1 IPNODE1
ucast eth1 IPNODE2
ucast eth0_rename IPNODE1
ucast eth0_rename IPNODE2
node node1
node node2
crm yes
\end{lstlisting}

\item - For 4 nodes configurations ::
\begin{lstlisting}
logfile /var/log/ha-log
debugfile /var/log/ha-debug
#use_logd on
udpport 694
keepalive 1 # 1 second
deadtime 10
initdead 80
ucast eth1 IPNODE1
ucast eth1 IPNODE2
ucast eth1 IPNODE3
ucast eth1 IPNODE4
ucast eth0_rename IPNODE1
ucast eth0_rename IPNODE2
ucast eth0_rename IPNODE3
ucast eth0_rename IPNODE4
node node1
node node2
node node3
node node4
crm yes
\end{lstlisting}
\end{itemize}

\textbf{\underline{details:}}\\
\textbf{keepalive} : x second between each beat\\
\textbf{deadtime} : after x second without heartbeat, the nodes is declared dead\\
\textbf{initdead} : set the time that it takes to declare a cluster node dead when Heartbeat is first started\\
\textbf{ucast}: [interface] [ip node1] network for send/receive heartbeat. You can also use multicast(mcast) or broadcast(bcast) address\\
\textbf{node} : the node names\\
\textbf{crm} : yes for use heartbeat version 2\\


\subsection{Ressource description file : \textit{/var/lib/heartbeat/crm/cib.xml}}
This file specifie the services manage by heartbeat. We can find three sections :
\begin{itemize}
\item The first one is the cluster property. It describe general cluster behavoir. We can specify different options :

\begin{itemize}
\item \textbf{symmetric-cluster} : Is resource can run on any node by default ?
\item \textbf{no-quorum-policy} : Action to do when cluster detect quorum
\item \textbf{default-resource-stickiness} : How much does ressource prefer to stay where it is
\item \textbf{default-resource-failure-stickiness} : How many failure before migration
\item \textbf{stonith-enabled} : If true, heartbeat kill the node which have resource failure. You must create a STONITH device before active it.
\item \textbf{tonith-action} : Action to do : reboot or poweroff
\item \textbf{startup-fencing} : STONITH unseen nodes if true (default)
\item \textbf{stop-orphan-resources} : If true, heartbeat should deleted resource be stoped
\item \textbf{stop-orphan-actions} : If true, should deleted action be cancelled
\item \textbf{remove-after-stop} : Remove resources from heartbeat after they are stopped, default = false
\item \textbf{short-resource-names} : true
\item \textbf{transition-idle-timeout} : Provides the default global timeout for actions. Any action which has a defined timeout automatically
uses the action-specific timeout
\item \textbf{default-action-timeout} : How long heartbeat wait for actions to compete
\item \textbf{is-managed-default} : Heartbeat can start or stop resources as required
\item \textbf{cluster-delay} : Round trip delay over the network
\item \textbf{pe-error-series-max} : The number of PE inputs resulting in ERRORs to save. -1 = unlimited
\item \textbf{pe-warn-series-max} : The number of PE inputs resulting in WARNINGs to save
\item \textbf{pe-input-series-max} : The number of other PE inputs to save
\end{itemize}

\item The second part is the resources properties. We can defined group of resources, and configure them.

\begin{itemize}
\item \textbf{Group definition} : you can create group with the directive :
\begin{lstlisting}[language=xml]
<group id="group1">
\end{lstlisting}
Now, you can add resources in this group.
\item \textbf{Resource definition} : There is two different type of ressource : OCF or LSB.\\
OCF example : 
\begin{lstlisting}[language=xml]
<primitive class="ocf" id="resource name" provider="heartbeat" type="resource-name">
\end{lstlisting}
LSB example : 
\begin{lstlisting}[language=xml]
<primitive class="lsb" id="oar-server" provider="heartbeat" type="oar-server">
\end{lstlisting}
OCF (Open Cluster Framework) Resource Agents is provided by heartbeat. You can find resource like IPaddr2 (for virtual IP), DRBDdisk for DRBD,etc.
You can also add ressource with LSB standard (Linux Standards Base). Generaly, it is the main type of resource. The location of the resource we can add is on \textit{/etc/ha.d/resource.d} and \textit{/etc/init.d/}.
When you defined the resource, the type is the name of the resource. Example, if you want to add mysql (/etc/init.d/mysql), you must write : type="mysql". For "id", you can enter the name of your choice.
\item \textbf{Resource operations} : Some operations can be added on resources. In our case, you can add resource monitoring. Example :
\begin{lstlisting}[language=xml]
<operations>
  <op id="VirtualIP-mon" interval="5s" name="monitor" timeout="5s"/>
</operations>
\end{lstlisting}
When the timeout is attained, heartbeat detect resource crash.
\item \textbf{Resources attribute} : Sometimes, resources required additional parameters. You can enter them with the attribute directive :
\begin{lstlisting}[language=xml]
<instance_attributes id="VirtualIP_inst_attr">
  <attributes>
    <nvpair id="VirtualIP_attr_0" name="ip" value="172.16.16.220"/>
    <nvpair id="VirtualIP_attr_1" name="nic" value="eth1"/>
    <nvpair id="VirtualIP_attr_2" name="cidr_netmask" value="24"/>
  </attributes>
</instance_attributes>
\end{lstlisting}

\end{itemize}

\item The last part is resources constaint. You can add contrainst on resource or resource group, like location constraint.
In our case, you can specify node priority, for example if you want resources run in priority on master.
\begin{lstlisting}[language=xml]
<constraints>
  <rsc_location id="rsc_location_group_1" rsc="group_1">
    <rule id="prefered_location_group_1" score="100">
      <expression attribute="#uname" id="prefered_location_group_1_expr" operation="eq" value="NODE-NAME"/>
    </rule>
   </rsc_location>
</constraints>
\end{lstlisting}

\item Example of heartbeat configuration cib.xml file : see appendix \ref{cibexample}


\end{itemize}

Now, you must set rights on the cib.xml file, and give it to its owner :
\begin{lstlisting}
chown hacluster:haclient /var/lib/heartbeat/crm/cib.xml
chmod 0600 /var/lib/heartbeat/crm/cib.xml
\end{lstlisting}




\subsection{Make OAR Service LSB compatible}
The OAR service (/etc/init.d/oar-server) is currently not compatible with LSB standard. It haven't the status function. So heartbeat can't monitor it.
For add status function on oar-server, edit \textit{/etc/init.d/oar-server} file, and add :
\begin{lstlisting}[language=bash]
status)
	if [ -f $PIDFILE ]
	then
		PID=`head -n 1 $PIDFILE`
		if [ `ps -ef | grep -v grep | grep $PID | wc -l` -ge 1 ]
		then
			echo "OAR is running"
			exit 0
		else
			echo "OAR is not runnig"
			exit 2
		fi
	else
		echo "OAR is not runnig"
		exit 2
	fi
	;;
\end{lstlisting}


\section{DRBD Configuration}
In DRBD configuration file, we can specifie a lots of parameters, which describe resources, synchronization type, synchronization rate, etc.
\subsection{synchronization type}
With have three synchronization type in DRBD :
\begin{itemize}
\item \textbf{Protocol A} : write ACK (on master) is send when data was transmitted on master disk and sent to slave.
\item \textbf{Protocol B} : write ACK (on master) is send when data was transmitted on master disk and received by slave.
\item \textbf{Protocol C} : write ACK (on master) is send when data was transmitted on master disk and slave disk.\\
\textbf{Very recommanded !}
\end{itemize}

\subsection{Startup action}
When DRBD is launched, we can do actions. For example, inform it to wait x seconds :
\begin{lstlisting}
startup {
  wfc-timeout x;
}
\end{lstlisting}

\subsection{Disk action}
The directive ``disk'' can add action to do for disk. It's recommanded to add this one :
\begin{lstlisting}
disk {
  on-io-error detach;
}
\end{lstlisting}

\subsection{Synchronization settings}
You can precise the rate of synchronization, the hot-area size :
\begin{lstlisting}
syncer {
  rate 700000K;
  al-extents 257;
}
\end{lstlisting}
1 extend correponds to 4 Mbytes, so with 257, we have 1 GBytes.

\subsection{Host section}
One resrouce must have two host section. A host section describe on each node which DRBD partition and disk is use. You must also specify neigboor ip and port :
\begin{lstlisting}
on node1 
  device DRBDPARTITION;
  disk DISKPARTITION;
  address node2:DBRDPORT;
  meta-disk internal;
}

on node2 
  device DRBDPARTITION;
  disk DISKPARTITION;
  address node1:DBRDPORT;
  meta-disk internal;
}
\end{lstlisting}

\subsection{Handlers}
DRBD provide an handler which can execute actions on evenements. For the problem of split brain (see \ref{splitbrain}),we can for exmaple write in log :
\begin{lstlisting}
handlers {
  split-brain "echo Splitbraindetected >> /var/log/drbd-log"
  pri-lost-after-sb "echo pri-lost-after-sb >> /var/log/drbd-log"
}

net {
  #cram-hmac-alg \"sha1\";
  #shared-secret \"123456\";
  after-sb-0pri discard-younger-primary;
  after-sb-1pri consensus;
  after-sb-2pri call-pri-lost-after-sb;
  #rr-conflict violently;
}
\end{lstlisting}
In the net directive, you can specifie different policie when you detect split brain.\\
\textbf{after-sb-0pri} : Split brain has just been detected, but at this time the resource is not in the Primary role on any host.
\begin{itemize}
\item disconnect : Do not recover automatically, call split-brain handler script, drop the connection and continue in disconnected mode.
\item discard-younger-primary. Discard and roll back the modifications made on the host which assumed the Primary role last.
\item discard-least-changes. Discard and roll back the modifications on the host where fewer changes occurred.
\item discard-zero-changes. If there is any host on which no changes occurred at all, simply apply all modifications made on the other and continue.
\end{itemize}
\textbf{after-sb-1pri} : Split brain has just been detected, and at this time the resource is in the Primary role on one host.
\begin{itemize}
\item disconnect 
\item consensus : Apply the same recovery policies as specified in after-sb-0pri. If a split brain victim can be selected after applying these policies, automatically resolve. Otherwise, behave exactly as if disconnect were specified.
\item call-pri-lost-after-sb. Apply the recovery policies as specified in after-sb-0pri. If a split brain victim can be selected after applying these policies, invoke the pri-lost-after-sb handler on the victim node.
\item discard-secondary. Whichever host is currently in the Secondary role, make that host the split brain victim.
\end{itemize}

\section{Database Configuration}

\subsection{Mysql}
\subsection{Postgresql}

\section{OAR Configuration}


\section{monitoring utils}


\chapter{known problems}
\section{Split brain}
\label{splitbrain} 

\chapter{Test}
















\appendix
\chapter{Heartbeat example configuration}
\label{cibexample}
\section {High Availability on OAR with 2 nodes configuration, postgresql database}
\begin{lstlisting}[language=xml]

<cib admin_epoch="0" have_quorum="true" ignore_dtd="false" num_peers="1" cib_feature_revision="2.0" generated="false" epoch="4" num_updates="4" cib-last-written="Mon Jul 20 15:22:44 2009" ccm_transition="1">
   <configuration>
   
   
     <crm_config>
       <cluster_property_set id="cib-bootstrap-options">
	 <attributes>
	   <nvpair id="cib-bootstrap-options-symmetric-cluster" name="symmetric-cluster" value="true"/>
	   <nvpair id="cib-bootstrap-options-no-quorum-policy" name="no-quorum-policy" value="stop"/>
	   <nvpair id="cib-bootstrap-options-default-resource-stickiness" name="default-resource-stickiness" value="0"/>
	   <nvpair id="cib-bootstrap-options-default-resource-failure-stickiness" name="default-resource-failure-stickiness" value="0"/>
	   <nvpair id="cib-bootstrap-options-stonith-enabled" name="stonith-enabled" value="false"/>
	   <nvpair id="cib-bootstrap-options-stonith-action" name="stonith-action" value="reboot"/>
	   <nvpair id="cib-bootstrap-options-startup-fencing" name="startup-fencing" value="true"/>
	   <nvpair id="cib-bootstrap-options-stop-orphan-resources" name="stop-orphan-resources" value="true"/>
	   <nvpair id="cib-bootstrap-options-stop-orphan-actions" name="stop-orphan-actions" value="true"/>
	   <nvpair id="cib-bootstrap-options-remove-after-stop" name="remove-after-stop" value="false"/>
	   <nvpair id="cib-bootstrap-options-short-resource-names" name="short-resource-names" value="true"/>
	   <nvpair id="cib-bootstrap-options-transition-idle-timeout" name="transition-idle-timeout" value="5min"/>
	   <nvpair id="cib-bootstrap-options-default-action-timeout" name="default-action-timeout" value="20s"/>
	   <nvpair id="cib-bootstrap-options-is-managed-default" name="is-managed-default" value="true"/>
	   <nvpair id="cib-bootstrap-options-cluster-delay" name="cluster-delay" value="60s"/>
	   <nvpair id="cib-bootstrap-options-pe-error-series-max" name="pe-error-series-max" value="-1"/>
	   <nvpair id="cib-bootstrap-options-pe-warn-series-max" name="pe-warn-series-max" value="-1"/>
	   <nvpair id="cib-bootstrap-options-pe-input-series-max" name="pe-input-series-max" value="-1"/>
	   <nvpair id="cib-bootstrap-options-dc-version" name="dc-version" value="2.1.3-node: 552305612591183b1628baa5bc6e903e0f1e26a3"/>
	 </attributes>
       </cluster_property_set>
     </crm_config>
     
     
     
     
     <resources>
       <group id="group_1">
	 <primitive class="ocf" id="VirtualIP" provider="heartbeat" type="IPaddr2">
	   <operations>
	     <op id="VirtualIP_mon" interval="5s" name="monitor" timeout="5s"/>
	   </operations>
	   <instance_attributes id="VirtualIP_inst_attr">
	     <attributes>
	       <nvpair id="VirtualIP_attr_0" name="ip" value="172.16.16.220"/>
	       <nvpair id="VirtualIP_attr_1" name="nic" value="eth1"/>
	       <nvpair id="VirtualIP_attr_2" name="cidr_netmask" value="24"/>
	     </attributes>
	   </instance_attributes>
	 </primitive>
	 <primitive class="heartbeat" id="DRBD-disk" provider="heartbeat" type="drbddisk">
	   <operations>
	     <op id="DRBD-disk_mon" interval="120s" name="monitor" timeout="60s"/>
	   </operations>
	   <instance_attributes id="DRBD-diskinst_attr">
	     <attributes>
	       <nvpair id="DRBD-disk_attr_1" name="1" value="mysql"/>
	     </attributes>
	   </instance_attributes>
	 </primitive>
	 <primitive class="ocf" id="Filesystem" provider="heartbeat" type="Filesystem">
	   <operations>
	     <op id="Filesystem_mon" interval="120s" name="monitor" timeout="60s"/>
	   </operations>
	   <instance_attributes id="Filesystem_inst_attr">
	     <attributes>
	       <nvpair id="Filesystem_attr_0" name="device" value="/dev/drbd0"/>
	       <nvpair id="Filesystem_attr_1" name="directory" value="/mnt/drbddata"/>
	       <nvpair id="Filesystem_attr_2" name="fstype" value="ext3"/>
	     </attributes>
	   </instance_attributes>
	 </primitive>
	 <primitive id="Postgres" class="lsb" type="postgresql-8.3" provider="heartbeat">
	   <operations>
	     <op id="Postgres_mon" interval="120s" name="monitor" timeout="60s"/>
	   </operations>
	   <meta_attributes id="Postgres_meta_attrs">
	     <attributes>
	       <nvpair id="Postgres_metaattr_target_role" name="target_role" value="started"/>
	     </attributes>
	   </meta_attributes>
	 </primitive>
	 <primitive class="lsb" id="oar-server" provider="heartbeat" type="oar-server">
	   <operations>
	     <op id="oar-server_mon" interval="120s" name="monitor" timeout="60s"/>
	   </operations>
	 </primitive>
	 <meta_attributes id="group_1_meta_attrs">
	   <attributes>
	     <nvpair id="group_1_metaattr_ordered" name="ordered" value="true"/>
	   </attributes>
	 </meta_attributes>
       </group>
     </resources>
     
     
     
     
     <constraints>
       <rsc_location id="rsc_location_group_1" rsc="group_1">
	 <rule id="prefered_location_group_1" score="100">
	   <expression attribute="#uname" id="prefered_location_group_1_expr" operation="eq" value="node1"/>
	 </rule>
       </rsc_location>
     </constraints>
     
     
   </configuration>
 </cib>
\end{lstlisting}


\section {High Availability on OAR with 4 nodes configuration}
\begin{lstlisting}[language=xml]
<cib generated="false" admin_epoch="0" have_quorum="false" ignore_dtd="false" num_peers="1" cib_feature_revision="2.0" epoch="72" num_updates="1" cib-last-written="Wed Jul 22 13:16:03 2009" ccm_transition="1">
   <configuration>


     <crm_config>
       <cluster_property_set id="cib-bootstrap-options">
         <attributes>
           <nvpair id="cib-bootstrap-options-dc-version" name="dc-version" value="2.1.3-node: 552305612591183b1628baa5bc6e903e0f1e26a3"/>
           <nvpair id="cib-bootstrap-options-symmetric-cluster" name="symmetric-cluster" value="false"/>
           <nvpair id="cib-bootstrap-options-no-quorum-policy" name="no-quorum-policy" value="ignore"/>
           <nvpair id="cib-bootstrap-options-default-resource-stickiness" name="default-resource-stickiness" value="0"/>
           <nvpair id="cib-bootstrap-options-default-resource-failure-stickiness" name="default-resource-failure-stickiness" value="0"/>
           <nvpair id="cib-bootstrap-options-stonith-enabled" name="stonith-enabled" value="false"/>
           <nvpair id="cib-bootstrap-options-stonith-action" name="stonith-action" value="reboot"/>
           <nvpair id="cib-bootstrap-options-startup-fencing" name="startup-fencing" value="true"/>
           <nvpair id="cib-bootstrap-options-stop-orphan-resources" name="stop-orphan-resources" value="true"/>
           <nvpair id="cib-bootstrap-options-stop-orphan-actions" name="stop-orphan-actions" value="true"/>
           <nvpair id="cib-bootstrap-options-remove-after-stop" name="remove-after-stop" value="false"/>
           <nvpair id="cib-bootstrap-options-short-resource-names" name="short-resource-names" value="true"/>
           <nvpair id="cib-bootstrap-options-transition-idle-timeout" name="transition-idle-timeout" value="5min"/>
           <nvpair id="cib-bootstrap-options-default-action-timeout" name="default-action-timeout" value="20s"/>
           <nvpair id="cib-bootstrap-options-is-managed-default" name="is-managed-default" value="true"/>
           <nvpair id="cib-bootstrap-options-cluster-delay" name="cluster-delay" value="60s"/>
           <nvpair id="cib-bootstrap-options-pe-error-series-max" name="pe-error-series-max" value="-1"/>
           <nvpair id="cib-bootstrap-options-pe-warn-series-max" name="pe-warn-series-max" value="-1"/>
           <nvpair id="cib-bootstrap-options-pe-input-series-max" name="pe-input-series-max" value="-1"/>
         </attributes>
       </cluster_property_set>
     </crm_config>




     <resources>
       <group id="Database-servers">
         <meta_attributes id="Database-servers_meta_attrs">
           <attributes>
             <nvpair name="target_role" id="Database-servers_metaattr_target_role" value="started"/>
             <nvpair id="Database-servers_metaattr_ordered" name="ordered" value="true"/>
             <nvpair id="Database-servers_metaattr_collocated" name="collocated" value="true"/>
           </attributes>
         </meta_attributes>
         <primitive class="ocf" type="IPaddr2" provider="heartbeat" id="VirtualIP-database">
           <operations>
             <op id="IPaddr2_mon" interval="5s" name="monitor" timeout="5s"/>
           </operations>
           <instance_attributes id="VirtualIP-database_instance_attrs">
             <attributes>
               <nvpair id="89b87cae-b271-4d55-8881-c89508ad8451" name="ip" value="172.16.16.221"/>
               <nvpair id="bff53b6b-a11d-4cb0-8db3-24ef1b82fbb0" name="nic" value="eth1"/>
               <nvpair id="d6dc4cd1-b057-48be-91c7-3b733e066269" name="cidr_netmask" value="24"/>
             </attributes>
           </instance_attributes>
           <meta_attributes id="VirtualIP-database_meta_attrs">
             <attributes>
               <nvpair name="target_role" id="VirtualIP-database_metaattr_target_role" value="started"/>
             </attributes>
           </meta_attributes>
         </primitive>
         <primitive id="DRBD-disk" class="heartbeat" type="drbddisk" provider="heartbeat">
           <operations>
             <op id="DRBD-disk_mon" interval="120s" name="monitor" timeout="60s"/>
           </operations>
           <instance_attributes id="DRBD-disk_inst_attr">
             <attributes>
               <nvpair id="DRBD-disk_attr_1" name="1" value="mysql"/>
             </attributes>
           </instance_attributes>
           <meta_attributes id="DRBD-disk_meta_attrs">
             <attributes>
               <nvpair id="DRBD-disk_metaattr_target_role" name="target_role" value="started"/>
             </attributes>
           </meta_attributes>
         </primitive>
         <primitive id="Filesystem" class="ocf" type="Filesystem" provider="heartbeat">
           <operations>
             <op id="Filesystem_mon" interval="120s" name="monitor" timeout="60s"/>
           </operations>
           <instance_attributes id="Filesystem_instance_attrs">
             <attributes>
               <nvpair id="29b7a79c-ad8f-47ef-91dd-610292acac10" name="device" value="/dev/drbd0"/>
               <nvpair id="639a3cc9-8df6-497a-b874-f04b72da5577" name="directory" value="/mnt/drbddata"/>
               <nvpair id="1e59d136-4bc7-4f6c-9eb4-a62e06ec48d1" name="fstype" value="ext3"/>
             </attributes>
           </instance_attributes>
           <meta_attributes id="Filesystem_meta_attrs">
             <attributes>
               <nvpair id="Filesystem_metaattr_target_role" name="target_role" value="started"/>
             </attributes>
           </meta_attributes>
         </primitive>
         <primitive id="MysqlHA" class="lsb" type="mysqlha" provider="heartbeat">
           <operations>
             <op id="MysqlHA_mon" interval="120s" name="monitor" timeout="60s"/>
           </operations>
           <meta_attributes id="MysqlHA_meta_attrs">
             <attributes>
               <nvpair id="MysqlHA_metaattr_target_role" name="target_role" value="started"/>
             </attributes>
           </meta_attributes>
         </primitive>
       </group>
       <group id="OAR-servers">
         <meta_attributes id="OAR-servers_meta_attrs">
           <attributes>
             <nvpair id="OAR-servers_metaattr_target_role" name="target_role" value="stopped"/>
             <nvpair id="OAR-servers_metaattr_ordered" name="ordered" value="true"/>
             <nvpair id="OAR-servers_metaattr_collocated" name="collocated" value="true"/>
           </attributes>
         </meta_attributes>
         <primitive id="VirtualIP-OAR-server" class="ocf" type="IPaddr2" provider="heartbeat">
           <operations>
             <op id="VirtualIP-OAR-server_mon" interval="5s" name="monitor" timeout="5s"/>
           </operations>
           <instance_attributes id="VirtualIP-OAR-server_instance_attrs">
             <attributes>
               <nvpair id="bb615f47-387e-46cb-945f-8757d494791b" name="ip" value="172.16.16.220"/>
               <nvpair id="6fe16a0e-c414-4a5b-8e5f-f0c48b369770" name="nic" value="eth1"/>
               <nvpair id="b25acc28-5186-4a22-9433-5347017cdc71" name="cidr_netmask" value="24"/>
             </attributes>
           </instance_attributes>
           <meta_attributes id="VirtualIP-OAR-server_meta_attrs">
             <attributes>
               <nvpair id="VirtualIP-OAR-server_metaattr_target_role" name="target_role" value="started"/>
             </attributes>
           </meta_attributes>
         </primitive>
         <primitive id="OAR-server" class="lsb" type="oar-server" provider="heartbeat">
           <operations>
             <op id="OAR-server_mon" interval="120s" name="monitor" timeout="60s"/>
           </operations>
           <meta_attributes id="OAR-server_meta_attrs">
             <attributes>
               <nvpair id="OAR-server_metaattr_target_role" name="target_role" value="started"/>
             </attributes>
           </meta_attributes>
         </primitive>
       </group>
     </resources>




     <constraints>
       <rsc_location id="location-Database" rsc="Database-servers">
         <rule id="prefered_location-Database" score="0">
           <expression attribute="#uname" id="f968a0dd-59dd-4d02-b54a-36c7139afa13" operation="ne" value="node1"/>
           <expression attribute="#uname" id="0ef6e38b-6a34-4e23-bc22-9dccfacc95b6" operation="ne" value="node3"/>
         </rule>
       </rsc_location>
       <rsc_location id="location-OAR" rsc="OAR-servers">
         <rule id="prefered_location-OAR" score="0">
           <expression attribute="#uname" id="4d2c21d8-aea7-460b-bfeb-cd6d22e4a17e" operation="ne" value="node2"/>
           <expression attribute="#uname" id="3292b1b4-04cb-4650-b6e0-2e4428c837a8" operation="ne" value="node4"/>
         </rule>
       </rsc_location>
       <rsc_location id="location_Master-OAR" rsc="OAR-servers">
         <rule id="prefered_location_Master-OAR" score="INFINITY">
           <expression attribute="#uname" id="37a60f84-c7ed-4e4a-835a-55382961c990" operation="eq" value="node1"/>
         </rule>
       </rsc_location>
       <rsc_location id="location_Master-Database" rsc="Database-servers">
         <rule id="prefered_location_Master-Database" score="INFINITY">
           <expression attribute="#uname" id="37a78f84-c7ed-4e4a-835a-54382451c990" operation="eq" value="node2"/>
         </rule>
       </rsc_location>
     </constraints>


   </configuration>
 </cib>
\end{lstlisting}

\end{document}          
